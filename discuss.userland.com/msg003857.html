<html>	<head>		<title>Re: Junk in URLs and link-rot</title>		</head>	<body>		<blockquote><blockquote>			<b>Archive of UserLand's first discussion group, started October 5, 1998.</b><hr>			<h2>Re: Junk in URLs and link-rot</h2>			<table cellpadding="0" cellspacing="5">				<tr><td><b>Author:</b></td><td>Jorn Barger</td></tr>				<tr><td><b>Posted:</b></td><td>3/9/1999; 5:12:42 PM</td></tr>				<tr><td><b>Topic:</b></td><td><a href="msg003808.html">Unique vs. Generic URLs</a></td></tr>				<tr><td><b>Msg #:</b></td><td>3857 (In response to <a href="msg003835.html">3835</a>)</td></tr>				<tr><td><b>Prev/Next:</b></td><td><a href="msg003856.html">3856</a> / <a href="msg003858.html">3858</a></td></tr>				</table>			<br><i>So, maybe you could explain more what you have in mind for the web-based database, and maybe someone around here could build it. </i><p>

Happy to!<p>

First, let's limit ourselves to sites that regularly publish new material (which I'll call webzines, for now).  Each of these has its own approach to when and where it announces new stories, and how it archives them, and how it arranges URLs and frames, and how it constructs URLs, and how its search-function works.<p>

So adding new zines to your daily surfing routine is a pain in the butt!  You have to painstakingly work out all these pieces, often by trial and error.  So most people end up limiting their surfing to only a few sites they visit routinely...<p>

But if there was a central web database that spelled out all these details in a machine-readable format, and if your browser could read that format, then ideally you could:<p>

- view a checklist of webzines and 'subscribe' to the ones that you want to check out.<p>

- the browser would know when and where to look for new material<p>

- it would know how to create a custom headlines-aggregation of all the new material in the zines you like best, even extracting the lead paragraph if you prefer this format<p>

- if URLs are hidden inside frames, it would know how to extract them, so you could have 'smart bookmarking' that ignored the 'volatile' URLs.<p>

- in cases where the URLs change when the articles are moved to the archive, this is usually according to some rule that could be encoded with a system like regexps.  'Smart bookmarks' might even remember both forms, and understand when they'll change.<p>

- if you get a 404 because a link wasn't 'smart' (or because a site decides to relocate everything), the same regexp-like rules could be applied to find where it's gone.<p>

- if it's just lost, the database should know how to format a search that will uncover it-- by filling in whatever bits you know about its content, date, author, etc.<p>

In my <a href="http://www.robotwisdom.com">weblog</a> I've been experimenting with "More" links that are generated by a Frontier script when I link an article.  I just have to pick a few key phrases from the article, and my "More" links currently can only go to AltaVista, but I'm learning to choose phrases that produce pure gold, even via clumsy old AV.  If the 'smart bookmark' was smart enough to help pick these phrases, and also to know how to send them to the zine's own search engine if there was one, then this would add another layer of 'linkrot insurance'.<p>

In my <a href="http://www.robotwisdom.com/web/parsing.html">parsing browsers</a> essay I compare all this to a netnews "newsrc" (which specifies which topical newsgroups you've subscribed to) plus killfile, where you subscribe, and say what you like and dislike, and all the other scutwork is then handled automatically.
			<br><br><hr><b>There are responses to this message:</b><ul><li>&nbsp;<a href="msg003861.html">Re: Junk in URLs and link-rot</a>, Dave Winer, 3/9/1999; 7:28:55 PM<p></ul>			<br><br><hr>This page was archived on 6/13/2001; 4:48:35 PM.<br><br>&copy; Copyright 1998-2001 <a href="http://www.userland.com/">UserLand Software</a>, Inc.			</blockquote></blockquote>		</body>	</html>